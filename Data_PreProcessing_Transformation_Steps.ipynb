{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hrxXPZWVO6tk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data transformation is a crucial step in any machine learning project. It involves preparing the data to make it suitable for training a machine learning model. Here are various common data transformation steps with code examples for each:\n",
        "\n",
        "# 1. Handling Missing Values\n",
        "# Missing values can be handled by imputing them with a statistic (e.g., mean, median) or by removing rows/columns with missing values."
      ],
      "metadata": {
        "id": "HG4Kq3C0O8u1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NlFUJmM-O8WF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT9pp2siOpnB",
        "outputId": "2c5a0c1e-d945-4bf4-8849-a73cbd39ae5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     A    B     C\n",
            "0  1.0  5.0   NaN\n",
            "1  2.0  NaN  10.0\n",
            "2  NaN  7.0  11.0\n",
            "3  4.0  8.0  12.0\n",
            "          A         B     C\n",
            "0  1.000000  5.000000  11.0\n",
            "1  2.000000  6.666667  10.0\n",
            "2  2.333333  7.000000  11.0\n",
            "3  4.000000  8.000000  12.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Sample data\n",
        "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8], 'C': [None, 10, 11, 12]}\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "\n",
        "# Imputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "\n",
        "print(df_imputed)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding Categorical Variables\n",
        "# Categorical variables need to be converted into a numerical format, such as\n",
        "# one-hot encoding or label encoding."
      ],
      "metadata": {
        "id": "Puz5M9hrPRrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "81pc6OWmOv78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "# Sample data\n",
        "data = {'Color': ['Red', 'Green', 'Blue', 'Green']}\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "\n",
        "# One-Hot Encoding\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "onehot_encoded = onehot_encoder.fit_transform(df[['Color']])\n",
        "df_onehot = pd.DataFrame(onehot_encoded, columns=onehot_encoder.get_feature_names_out(['Color']))\n",
        "\n",
        "print(df_onehot)\n",
        "\n",
        "# Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "df['Color_Encoded'] = label_encoder.fit_transform(df['Color'])\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIKL_nZPPZs4",
        "outputId": "9600fd8c-481d-43a5-b293-8d2fe8a8a379"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Color\n",
            "0    Red\n",
            "1  Green\n",
            "2   Blue\n",
            "3  Green\n",
            "   Color_Blue  Color_Green  Color_Red\n",
            "0         0.0          0.0        1.0\n",
            "1         0.0          1.0        0.0\n",
            "2         1.0          0.0        0.0\n",
            "3         0.0          1.0        0.0\n",
            "   Color  Color_Encoded\n",
            "0    Red              2\n",
            "1  Green              1\n",
            "2   Blue              0\n",
            "3  Green              1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K_-hMW2vPqVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Feature Scaling\n",
        "# Feature scaling standardizes the range of independent variables or features. # Common methods are normalization and standardization.\n"
      ],
      "metadata": {
        "id": "nO6UNP1nTOyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Sample data\n",
        "data = {'Height': [150, 160, 170, 180], 'Weight': [65, 70, 75, 80]}\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "print(df_standardized)\n",
        "\n",
        "# Normalization\n",
        "min_max_scaler = MinMaxScaler()\n",
        "df_normalized = pd.DataFrame(min_max_scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "print(df_normalized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYn1vplBTSnF",
        "outputId": "be6bddc0-7e99-4374-b9c0-3731976647ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Height  Weight\n",
            "0     150      65\n",
            "1     160      70\n",
            "2     170      75\n",
            "3     180      80\n",
            "     Height    Weight\n",
            "0 -1.341641 -1.341641\n",
            "1 -0.447214 -0.447214\n",
            "2  0.447214  0.447214\n",
            "3  1.341641  1.341641\n",
            "     Height    Weight\n",
            "0  0.000000  0.000000\n",
            "1  0.333333  0.333333\n",
            "2  0.666667  0.666667\n",
            "3  1.000000  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "# Creating new features from existing data can improve model performance. This # can involve mathematical transformations, aggregations, etc."
      ],
      "metadata": {
        "id": "vW0dIu38T1z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = {'Height': [150, 160, 170, 180], 'Weight': [65, 70, 75, 80]}\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "\n",
        "# Creating a new feature: Body Mass Index (BMI)\n",
        "df['BMI'] = df['Weight'] / (df['Height'] / 100) ** 2\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL1ZaSXqTZ7P",
        "outputId": "18f84c7d-c9ef-4260-8856-0100e9796fa4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Height  Weight\n",
            "0     150      65\n",
            "1     160      70\n",
            "2     170      75\n",
            "3     180      80\n",
            "   Height  Weight        BMI\n",
            "0     150      65  28.888889\n",
            "1     160      70  27.343750\n",
            "2     170      75  25.951557\n",
            "3     180      80  24.691358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NOsEWO6OT9_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection\n",
        "# Selecting the most important features can reduce overfitting and improve model performance."
      ],
      "metadata": {
        "id": "UhY3odhcUCu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# Sample data\n",
        "X = [[0.87, 1.0, 3.1], [-1.34, -0.52, -2.3], [0.31, 1.21, 1.7], [0.24, 0.1, -1.4]]\n",
        "y = [0, 1, 0, 1]\n",
        "\n",
        "# Feature Selection\n",
        "clf = RandomForestClassifier(n_estimators=50)\n",
        "clf = clf.fit(X, y)\n",
        "\n",
        "model = SelectFromModel(clf, prefit=True)\n",
        "X_new = model.transform(X)\n",
        "\n",
        "print(X_new)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RjZKTr9UFFx",
        "outputId": "ddf7a052-1574-434f-f81e-0fde961b474e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.  ]\n",
            " [-0.52]\n",
            " [ 1.21]\n",
            " [ 0.1 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GWQPoLeRUHSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation\n",
        "# Especially useful for image and text data, data augmentation involves generating new data points by applying transformations such as rotations, translations, and flips."
      ],
      "metadata": {
        "id": "UOv0j0vkUVZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Sample image data\n",
        "image = Image.open('sample_image.jpg')\n",
        "data = np.expand_dims(np.array(image), axis=0)\n",
        "\n",
        "# Image Data Generator\n",
        "datagen = ImageDataGenerator(rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
        "\n",
        "# Generating augmented images\n",
        "aug_iter = datagen.flow(data, batch_size=1)\n",
        "aug_images = [next(aug_iter)[0].astype(np.uint8) for _ in range(5)]\n",
        "\n",
        "for aug_image in aug_images:\n",
        "    Image.fromarray(aug_image).show()\n"
      ],
      "metadata": {
        "id": "A16MFfy3UXaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality Reduction\n",
        "# Reducing the number of features can help with the performance of machine\n",
        "# learning models. Principal Component Analysis (PCA) is a common technique.\n",
        "\n"
      ],
      "metadata": {
        "id": "DKktVgP0U7TU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Sample data\n",
        "data = [[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0]]\n",
        "df = pd.DataFrame(data, columns=['x', 'y'])\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=1)\n",
        "df_reduced = pca.fit_transform(df)\n",
        "\n",
        "print(df_reduced)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BipgXOwxVbbf",
        "outputId": "2f48273a-5d93-47a0-86a5-a84054066bde"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.44362444]\n",
            " [ 2.17719404]\n",
            " [-0.57071239]\n",
            " [ 0.12902465]\n",
            " [-1.29188186]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('maintenance_data.csv')\n",
        "\n",
        "# Handling missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df[['temperature', 'pressure']] = imputer.fit_transform(df[['temperature', 'pressure']])\n",
        "\n",
        "# Encoding categorical variables\n",
        "categorical_features = ['machine_type', 'failure_type']\n",
        "onehot_encoder = OneHotEncoder()\n",
        "df_encoded = pd.get_dummies(df, columns=categorical_features)\n",
        "\n",
        "# Feature engineering\n",
        "df_encoded['runtime_between_failures'] = df_encoded['total_runtime'] / (df_encoded['failure_count'] + 1)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "df_encoded[['temperature', 'pressure', 'runtime_between_failures']] = scaler.fit_transform(df_encoded[['temperature', 'pressure', 'runtime_between_failures']])\n",
        "\n",
        "# Splitting data\n",
        "X = df_encoded.drop('failure', axis=1)\n",
        "y = df_encoded['failure']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Dimensionality reduction\n",
        "pca = PCA(n_components=10)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Model training and evaluation\n",
        "model = RandomForestClassifier(n_estimators=100)\n",
        "model.fit(X_train_pca, y_train)\n",
        "scores = cross_val_score(model, X_test_pca, y_test, cv=5)\n",
        "\n",
        "print(f'Cross-validated scores: {scores}')\n",
        "print(f'Mean accuracy: {np.mean(scores)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYpPA56xVc2K",
        "outputId": "20b0544b-0c37-4163-8b31-66977ee1005d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated scores: [0.5 1.  1.  1.  1. ]\n",
            "Mean accuracy: 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "\n",
        "# Load dataset from seaborn library\n",
        "import seaborn as sns\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Drop columns that won't be used in this example\n",
        "titanic.drop(columns=['deck', 'embark_town', 'alive', 'class', 'who', 'adult_male', 'alone'], inplace=True)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(titanic.head())\n",
        "\n",
        "# Handling missing values\n",
        "# Impute 'age' with the median and 'embarked' with the most frequent value\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Apply the imputers\n",
        "titanic['age'] = num_imputer.fit_transform(titanic[['age']]).ravel()\n",
        "titanic['embarked'] = cat_imputer.fit_transform(titanic[['embarked']]).ravel()\n",
        "\n",
        "# Check for remaining missing values\n",
        "print(titanic.isnull().sum())\n",
        "\n",
        "# Drop rows with missing 'fare' (if any)\n",
        "titanic.dropna(subset=['fare'], inplace=True)\n",
        "\n",
        "# Encoding categorical variables\n",
        "# One-hot encode 'sex' and 'embarked'\n",
        "titanic = pd.get_dummies(titanic, columns=['sex', 'embarked'], drop_first=True)\n",
        "\n",
        "# Feature engineering\n",
        "# Create a new feature 'family_size'\n",
        "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1\n",
        "\n",
        "# Drop columns that are now redundant or not needed\n",
        "titanic.drop(columns=['sibsp', 'parch'], inplace=True)\n",
        "\n",
        "# Define features and target variable\n",
        "X = titanic.drop(columns=['survived'])\n",
        "y = titanic['survived']\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X[['age', 'fare', 'family_size']] = scaler.fit_transform(X[['age', 'fare', 'family_size']])\n",
        "\n",
        "# Splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model training and evaluation\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "scores = cross_val_score(model, X_test, y_test, cv=5)\n",
        "\n",
        "print(f'Cross-validated scores: {scores}')\n",
        "print(f'Mean accuracy: {np.mean(scores)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fd4NSsPX9y3",
        "outputId": "fcdfc4d2-0c68-4f28-efc1-5458c1ad662d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   survived  pclass     sex   age  sibsp  parch     fare embarked\n",
            "0         0       3    male  22.0      1      0   7.2500        S\n",
            "1         1       1  female  38.0      1      0  71.2833        C\n",
            "2         1       3  female  26.0      0      0   7.9250        S\n",
            "3         1       1  female  35.0      1      0  53.1000        S\n",
            "4         0       3    male  35.0      0      0   8.0500        S\n",
            "survived    0\n",
            "pclass      0\n",
            "sex         0\n",
            "age         0\n",
            "sibsp       0\n",
            "parch       0\n",
            "fare        0\n",
            "embarked    0\n",
            "dtype: int64\n",
            "Cross-validated scores: [0.80555556 0.86111111 0.77777778 0.86111111 0.85714286]\n",
            "Mean accuracy: 0.8325396825396826\n"
          ]
        }
      ]
    }
  ]
}