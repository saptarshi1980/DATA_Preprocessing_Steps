# -*- coding: utf-8 -*-
"""Data_PreProcessing_Transformation_Steps.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iMXdew83hBLby-aYdI7f_E8mW8HZmFvc

# Data transformation is a crucial step in any machine learning project. It involves preparing the data to make it suitable for training a machine learning model. Here are various common data transformation steps with code examples for each:

# 1. Handling Missing Values
# Missing values can be handled by imputing them with a statistic (e.g., mean, median) or by removing rows/columns with missing values.
"""

import pandas as pd
from sklearn.impute import SimpleImputer

# Sample data
data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8], 'C': [None, 10, 11, 12]}
df = pd.DataFrame(data)
print(df)

# Imputer
imputer = SimpleImputer(strategy='mean')
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

print(df_imputed)

"""# Encoding Categorical Variables
# Categorical variables need to be converted into a numerical format, such as
# one-hot encoding or label encoding.
"""

from sklearn.preprocessing import OneHotEncoder, LabelEncoder

# Sample data
data = {'Color': ['Red', 'Green', 'Blue', 'Green']}
df = pd.DataFrame(data)
print(df)

# One-Hot Encoding
onehot_encoder = OneHotEncoder(sparse=False)
onehot_encoded = onehot_encoder.fit_transform(df[['Color']])
df_onehot = pd.DataFrame(onehot_encoded, columns=onehot_encoder.get_feature_names_out(['Color']))

print(df_onehot)

# Label Encoding
label_encoder = LabelEncoder()
df['Color_Encoded'] = label_encoder.fit_transform(df['Color'])

print(df)



"""# 3. Feature Scaling
# Feature scaling standardizes the range of independent variables or features. # Common methods are normalization and standardization.

"""

from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Sample data
data = {'Height': [150, 160, 170, 180], 'Weight': [65, 70, 75, 80]}
df = pd.DataFrame(data)
print(df)
# Standardization
scaler = StandardScaler()
df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)

print(df_standardized)

# Normalization
min_max_scaler = MinMaxScaler()
df_normalized = pd.DataFrame(min_max_scaler.fit_transform(df), columns=df.columns)

print(df_normalized)

"""# Feature Engineering
# Creating new features from existing data can improve model performance. This # can involve mathematical transformations, aggregations, etc.
"""

# Sample data
data = {'Height': [150, 160, 170, 180], 'Weight': [65, 70, 75, 80]}
df = pd.DataFrame(data)
print(df)

# Creating a new feature: Body Mass Index (BMI)
df['BMI'] = df['Weight'] / (df['Height'] / 100) ** 2

print(df)



"""# Feature Selection
# Selecting the most important features can reduce overfitting and improve model performance.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel

# Sample data
X = [[0.87, 1.0, 3.1], [-1.34, -0.52, -2.3], [0.31, 1.21, 1.7], [0.24, 0.1, -1.4]]
y = [0, 1, 0, 1]

# Feature Selection
clf = RandomForestClassifier(n_estimators=50)
clf = clf.fit(X, y)

model = SelectFromModel(clf, prefit=True)
X_new = model.transform(X)

print(X_new)



"""# Data Augmentation
# Especially useful for image and text data, data augmentation involves generating new data points by applying transformations such as rotations, translations, and flips.
"""

from keras.preprocessing.image import ImageDataGenerator
import numpy as np
from PIL import Image

# Sample image data
image = Image.open('sample_image.jpg')
data = np.expand_dims(np.array(image), axis=0)

# Image Data Generator
datagen = ImageDataGenerator(rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')

# Generating augmented images
aug_iter = datagen.flow(data, batch_size=1)
aug_images = [next(aug_iter)[0].astype(np.uint8) for _ in range(5)]

for aug_image in aug_images:
    Image.fromarray(aug_image).show()

"""# Dimensionality Reduction
# Reducing the number of features can help with the performance of machine
# learning models. Principal Component Analysis (PCA) is a common technique.


"""

from sklearn.decomposition import PCA

# Sample data
data = [[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0]]
df = pd.DataFrame(data, columns=['x', 'y'])

# PCA
pca = PCA(n_components=1)
df_reduced = pca.fit_transform(df)

print(df_reduced)

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, cross_val_score

# Load dataset
df = pd.read_csv('maintenance_data.csv')

# Handling missing values
imputer = SimpleImputer(strategy='mean')
df[['temperature', 'pressure']] = imputer.fit_transform(df[['temperature', 'pressure']])

# Encoding categorical variables
categorical_features = ['machine_type', 'failure_type']
onehot_encoder = OneHotEncoder()
df_encoded = pd.get_dummies(df, columns=categorical_features)

# Feature engineering
df_encoded['runtime_between_failures'] = df_encoded['total_runtime'] / (df_encoded['failure_count'] + 1)

# Feature scaling
scaler = StandardScaler()
df_encoded[['temperature', 'pressure', 'runtime_between_failures']] = scaler.fit_transform(df_encoded[['temperature', 'pressure', 'runtime_between_failures']])

# Splitting data
X = df_encoded.drop('failure', axis=1)
y = df_encoded['failure']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Dimensionality reduction
pca = PCA(n_components=10)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Model training and evaluation
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train_pca, y_train)
scores = cross_val_score(model, X_test_pca, y_test, cv=5)

print(f'Cross-validated scores: {scores}')
print(f'Mean accuracy: {np.mean(scores)}')

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, cross_val_score

# Load dataset from seaborn library
import seaborn as sns
titanic = sns.load_dataset('titanic')

# Drop columns that won't be used in this example
titanic.drop(columns=['deck', 'embark_town', 'alive', 'class', 'who', 'adult_male', 'alone'], inplace=True)

# Display the first few rows of the dataset
print(titanic.head())

# Handling missing values
# Impute 'age' with the median and 'embarked' with the most frequent value
num_imputer = SimpleImputer(strategy='median')
cat_imputer = SimpleImputer(strategy='most_frequent')

# Apply the imputers
titanic['age'] = num_imputer.fit_transform(titanic[['age']]).ravel()
titanic['embarked'] = cat_imputer.fit_transform(titanic[['embarked']]).ravel()

# Check for remaining missing values
print(titanic.isnull().sum())

# Drop rows with missing 'fare' (if any)
titanic.dropna(subset=['fare'], inplace=True)

# Encoding categorical variables
# One-hot encode 'sex' and 'embarked'
titanic = pd.get_dummies(titanic, columns=['sex', 'embarked'], drop_first=True)

# Feature engineering
# Create a new feature 'family_size'
titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1

# Drop columns that are now redundant or not needed
titanic.drop(columns=['sibsp', 'parch'], inplace=True)

# Define features and target variable
X = titanic.drop(columns=['survived'])
y = titanic['survived']

# Feature scaling
scaler = StandardScaler()
X[['age', 'fare', 'family_size']] = scaler.fit_transform(X[['age', 'fare', 'family_size']])

# Splitting data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model training and evaluation
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
scores = cross_val_score(model, X_test, y_test, cv=5)

print(f'Cross-validated scores: {scores}')
print(f'Mean accuracy: {np.mean(scores)}')